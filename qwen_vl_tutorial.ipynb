{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen Vision Language Model (VLM) Tutorial\n",
    "\n",
    "This notebook will guide you through using the Qwen2.5-VL model for multimodal tasks. The Qwen2.5-VL model can process both text and images, allowing for tasks like image captioning, visual question answering, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's make sure we have all the necessary packages installed. We've already set up the environment using `uv`, but you can run this cell if you need to install any additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install packages\n",
    "# !uv pip install torch transformers jupyter pillow accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Qwen2.5-VL Model\n",
    "\n",
    "We'll load the Qwen2.5-VL-3B-Instruct model, which is fine-tuned for instruction following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645226728d6d4fb0a625458022e151b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4070dd16134a49b69dd1a4e431114ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b65c1decb74d5f9dd226280291d764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e8e88877c6491ebb2521daa3920d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7415da989e408f9b84126b5606465f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/78/d5/78d529a803afa4c032209fed045af4e48786044483b74ac0fb42617eb5646875/41a8895c164b4d32bae6b302f4603fcbc1797f32dafa45c7e9bcda23c6755df8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00002.safetensors%3B+filename%3D%22model-00001-of-00002.safetensors%22%3B&Expires=1743288618&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzI4ODYxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc4L2Q1Lzc4ZDUyOWE4MDNhZmE0YzAzMjIwOWZlZDA0NWFmNGU0ODc4NjA0NDQ4M2I3NGFjMGZiNDI2MTdlYjU2NDY4NzUvNDFhODg5NWMxNjRiNGQzMmJhZTZiMzAyZjQ2MDNmY2JjMTc5N2YzMmRhZmE0NWM3ZTliY2RhMjNjNjc1NWRmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=SFIb6CmZ6AR1zWhzM2a5ZkxJX22IKJ1hrcUvsDPA7vDFs3sw-qVCJKpR4Bn82U7QipsXkxkrRv4pfRmvECa6y-rBGUhcbLFglT1tPJpn5Se80y69NjUAJ5ZO5WDb0ioNaC5w7nhGJcx1TCazNoUKkUptUeTbipOdCQX3MshUzMMNIwhrhmnyhQLaGQytY87n-Ym5B8hFwb120xxP4DCi%7EREGpVK2obl0dKbhkrLFEx35aLhPoAUfi7aLzQuEYZGBMmXl38VgtQMB9PT79ozd1%7Eb2frgBTOxn1I7Mb%7EDxaLHM7S1rMVGfkVqmMuLN-QN4eU-RGixxN5aohrqPi0bQBA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/78/d5/78d529a803afa4c032209fed045af4e48786044483b74ac0fb42617eb5646875/365531ff8752420e89dee707b79d021fb2d6e25abafe486f080555a4fe6972e4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00002.safetensors%3B+filename%3D%22model-00002-of-00002.safetensors%22%3B&Expires=1743288618&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzI4ODYxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc4L2Q1Lzc4ZDUyOWE4MDNhZmE0YzAzMjIwOWZlZDA0NWFmNGU0ODc4NjA0NDQ4M2I3NGFjMGZiNDI2MTdlYjU2NDY4NzUvMzY1NTMxZmY4NzUyNDIwZTg5ZGVlNzA3Yjc5ZDAyMWZiMmQ2ZTI1YWJhZmU0ODZmMDgwNTU1YTRmZTY5NzJlND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=bNh2XKLeX2WB0Ujooex0E4j4fTwUDzbevbianVYTvnxdUdnesGarKzTPtpMRZGw%7EkBa42k5HNUURv-ZxIJlXt12lkNmPcmDicciHW9QSBnrXPsboOATyctuAaXUCfL3jgwUj5orHeLh55ASjNntRQlVRWAVdfGtyvagq7WBfiumaVpzZNxCW73rXvC%7EMCvJrl32aKLYwB4Z1x2bvycqOMtw8STMm6YxJ6XGVisSJHRacJK9T1SKLsbnuzuhLd3FMB9nRLL24qLwhvm%7E%7EGZaUJrM7LHeaTWnZgiqg5mBBZpBj9%7EKNpAWsOLk-Ek7VWvHTCfdNc2gXHm2KUjF5r%7EqZtg__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7059433ed8a745db89999a04b5cbcf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  54%|#####4    | 1.91G/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f550aacbce4420b98d4296a5ba1cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  50%|#####     | 2.00G/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Advanced Model Loading (Optional)\n",
    "\n",
    "For better performance, you can enable flash attention 2. This is especially useful for multi-image and video scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use flash attention 2 for better performance\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Custom Processor Configuration (Optional)\n",
    "\n",
    "You can customize the processor to control the number of visual tokens per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to customize the processor\n",
    "# min_pixels = 256*28*28  # Minimum number of pixels\n",
    "# max_pixels = 1280*28*28  # Maximum number of pixels\n",
    "# processor = AutoProcessor.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "#     min_pixels=min_pixels, \n",
    "#     max_pixels=max_pixels\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_url):\n",
    "    \"\"\"Display an image from a URL\"\"\"\n",
    "    if image_url.startswith('http'):\n",
    "        response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(image_url)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return img\n",
    "\n",
    "def generate_response(messages):\n",
    "    \"\"\"Generate a response from the model given messages\"\"\"\n",
    "    # Prepare for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    # Generate the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example 1: Image Description\n",
    "\n",
    "Let's start with a simple example: asking the model to describe an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image URL\n",
    "image_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "\n",
    "# Display the image\n",
    "img = display_image(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages for image description\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_url,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example 2: Visual Question Answering\n",
    "\n",
    "Now let's try asking specific questions about an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages for visual question answering\n",
    "vqa_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_url,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What animals can you see in this image?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(vqa_messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example 3: Using a Local Image\n",
    "\n",
    "You can also use local images with the model. First, you'll need to download an image or use one you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image\n",
    "!curl -o sample_image.jpg https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the local image\n",
    "local_img = display_image(\"sample_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages using the local image\n",
    "local_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"sample_image.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What is the main color of the background?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(local_messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example 4: Multi-turn Conversation\n",
    "\n",
    "The model can also handle multi-turn conversations about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_url,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# First response\n",
    "response1 = generate_response(conversation)\n",
    "print(\"Model:\", response1)\n",
    "\n",
    "# Add the model's response to the conversation\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response1})\n",
    "\n",
    "# Add a follow-up question\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you count how many animals are there?\"})\n",
    "\n",
    "# Get the model's response to the follow-up\n",
    "response2 = generate_response(conversation)\n",
    "print(\"\\nUser: Can you count how many animals are there?\")\n",
    "print(\"Model:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example 5: Multiple Images\n",
    "\n",
    "The model can also process multiple images in a single conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a second image\n",
    "!curl -o sample_image2.jpg https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/assets/demo2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the second image\n",
    "img2 = display_image(\"sample_image2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages with multiple images\n",
    "multi_image_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Compare these two images:\"},\n",
    "            {\"type\": \"image\", \"image\": \"sample_image.jpg\"},\n",
    "            {\"type\": \"image\", \"image\": \"sample_image2.jpg\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(multi_image_messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Customizing Generation Parameters\n",
    "\n",
    "You can customize the generation parameters to control the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_params(messages, max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate a response with custom parameters\"\"\"\n",
    "    # Prepare for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    # Generate with custom parameters\n",
    "    generated_ids = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages for creative generation\n",
    "creative_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_url,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Write a short story inspired by this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate with different parameters\n",
    "print(\"=== More Creative (Higher Temperature) ===\")\n",
    "creative_response = generate_with_params(creative_messages, temperature=1.0, max_new_tokens=200)\n",
    "print(creative_response)\n",
    "\n",
    "print(\"\\n=== More Focused (Lower Temperature) ===\")\n",
    "focused_response = generate_with_params(creative_messages, temperature=0.3, max_new_tokens=200)\n",
    "print(focused_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've explored how to use the Qwen2.5-VL model for various multimodal tasks:\n",
    "\n",
    "1. Image description\n",
    "2. Visual question answering\n",
    "3. Using local images\n",
    "4. Multi-turn conversations\n",
    "5. Processing multiple images\n",
    "6. Customizing generation parameters\n",
    "\n",
    "The Qwen2.5-VL model is a powerful tool for multimodal tasks, capable of understanding both text and images to generate meaningful responses. You can explore more advanced use cases and fine-tuning options in the [official documentation](https://github.com/QwenLM/Qwen2.5)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
